# Техническое видение проекта

Данный документ описывает техническое видение проекта, служит отправной точкой и техническим проектом для последующей разработки.

## Технологии

- **Язык программирования**: Python
- **Интеграция с LLM**: OpenRouter через OpenAI клиент
- **Хранение данных**: Встроенные структуры Python (dict/list) для хранения истории диалога
- **Мессенджер-интеграция**: Telegram API через aiogram
- **Тестирование**: pytest
- **Управление зависимостями**: uv
- **Автоматизация**: Make (Makefile)
- **Деплой**: Docker

## Принцип разработки

- **KISS (Keep It Simple, Stupid)**: Максимальная простота решения, отсутствие оверинжиниринга
- **Итеративный подход**: Короткие циклы разработки с быстрым получением обратной связи
- **Функциональный стиль**: Организация кода в функциональном стиле без использования ООП
- **Модульность**: Разделение кода на независимые модули с четкими интерфейсами
- **Минимализм**: Использование только необходимого минимума кода и зависимостей
- **GitHub Flow**: Простой подход к управлению версиями через ветки и pull request'ы

## Структура проекта

```
/
├── src/                  # Исходный код
│   ├── bot/              # Модуль для работы с Telegram
│   ├── llm/              # Модуль для работы с LLM
│   ├── dialog/           # Модуль для управления диалогами
│   └── main.py           # Точка входа в приложение
├── tests/                # Тесты
├── doc/                  # Документация
├── Dockerfile            # Файл для сборки Docker-образа
├── Makefile              # Команды для автоматизации
├── requirements.txt      # Зависимости проекта
└── README.md             # Описание проекта
```

## Архитектура проекта

Архитектура проекта основана на принципах простоты и функционального подхода:

### Слои приложения
- **Слой взаимодействия с пользователем**: Telegram бот на базе aiogram
- **Слой бизнес-логики**: Обработка диалогов и управление контекстом
- **Слой интеграции с LLM**: Взаимодействие с OpenRouter через OpenAI клиент

### Поток данных
1. Пользователь отправляет сообщение в Telegram
2. Бот получает сообщение и передает в обработчик диалогов
3. Обработчик диалогов формирует запрос к LLM
4. LLM-модуль отправляет запрос к OpenRouter и получает ответ
5. Ответ возвращается через обработчик диалогов обратно пользователю

### Основные компоненты
- **Telegram Bot Handler**: Обработка сообщений из Telegram
- **Dialog Manager**: Управление контекстом диалога
- **LLM Client**: Взаимодействие с языковой моделью

## Модель данных

Все данные хранятся в памяти с использованием стандартных структур Python:

### Структура диалога
```python
dialog = {
    "user_id": "123456789",
    "start_time": "2023-06-15T10:30:00",
    "messages": [
        # Список сообщений
    ]
}
```

### Формат сообщений
```python
message = {
    "role": "user",  # "system" и "assistant" для сообщений от системы и ассистента
    "content": "Текст сообщения",
    "timestamp": "2023-06-15T10:31:00"
}
```

### Контекст пользователя
```python
user_context = {
    "user_id": "123456789",
    "current_dialog_id": "dialog_123",
    "settings": {
        # Пользовательские настройки
    }
}
```

## Работа с LLM

### Интеграция с OpenRouter
- Использование OpenAI-совместимого клиента для взаимодействия с OpenRouter
- Конфигурация базовых параметров запроса:
  - Модель: выбор оптимальной модели по соотношению цена/качество
  - Температура: 0.7 (баланс между креативностью и точностью)
  - Max tokens: 1000 (ограничение для контроля расходов)
  - Top-p: 0.9 (разнообразие ответов)

### Формирование промптов
```python
def create_prompt(user_message, history, system_prompt):
    messages = [
        {"role": "system", "content": system_prompt},
        # Добавление истории диалога
        *history,
        # Добавление текущего сообщения пользователя
        {"role": "user", "content": user_message}
    ]
    return messages
```

### Обработка ответов
- Базовая валидация ответов от LLM
- Обработка ошибок API и повторные попытки с экспоненциальной задержкой
- Фильтрация неприемлемого контента

## Мониторинг LLM

### Логирование взаимодействий
- Базовое логирование запросов к LLM и полученных ответов
- Отслеживание и логирование ошибок при взаимодействии с API
- Сохранение временных меток для анализа производительности

## Сценарии работы

### Начало взаимодействия
- Пользователь запускает бота командой `/start`
- Бот отправляет приветственное сообщение с инструкциями по использованию

### Обработка текстовых запросов
- Пользователь отправляет текстовое сообщение
- Бот формирует контекст из истории диалога
- Запрос отправляется к LLM через OpenRouter
- Полученный ответ отправляется пользователю

### Управление контекстом
- Бот автоматически сохраняет историю диалога для поддержания контекста
- Пользователь может сбросить контекст и начать новый диалог командой `/reset`

## Деплой

### Контейнеризация
- Создание простого Docker-образа с приложением
- Минимальный набор зависимостей для работы бота

### Запуск и управление
- Запуск контейнера напрямую через Docker CLI
- Конфигурация через файл .env и переменные окружения

### Развертывание
- Локальное развертывание на собственной машине
- Простой процесс запуска и остановки через скрипты

## Подход к конфигурированию

### Переменные окружения
- Хранение чувствительных данных (API ключи, токены)

### Файл .env
- Локальное хранение переменных окружения

### Конфигурационные константы
- Базовые настройки в коде
- Параметры по умолчанию для неуказанных значений

## Подход к логгированию

### Стандартная библиотека logging
- Использование встроенного модуля Python для логгирования
- Настройка уровней логгирования (INFO, ERROR, DEBUG)

### Форматирование логов
- Структурированные логи с временными метками
- Информация о контексте выполнения

### Вывод логов
- Вывод в консоль для разработки
- Запись в файл для продакшена


